{
    "players": [
        {
            "name": "Angel",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM5",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Winter",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM0",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Adrian",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM12",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Logan",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM10",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Eden",
            "is_mafia": true,
            "is_llm": true,
            "real_name": "LLM8",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Emerson",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM6",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Ziggy",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM3",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Peyton",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM4",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Quinn",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM13",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Gray",
            "is_mafia": true,
            "is_llm": true,
            "real_name": "LLM14",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Kai",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM9",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Mickey",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM7",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Rowan",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM2",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "River",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM11",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Skylar",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM1",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        },
        {
            "name": "Drew",
            "is_mafia": false,
            "is_llm": true,
            "real_name": "LLM15",
            "llm_config": {
                "model_name": "o4-mini",
                "use_together": false,
                "use_pipeline": false,
                "use_openai": true,
                "pipeline_task": "text-generation",
                "max_new_tokens": 25,
                "max_tokens": 25,
                "num_beams": 1,
                "repetition_penalty": 1.25,
                "do_sample": true,
                "temperature": 1.3,
                "no_repeat_ngram_size": 8,
                "num_words_per_second_to_wait": 2,
                "pass_turn_token": "<wait>",
                "use_turn_token": "<send>",
                "async_type": "schedule_then_generate"
            }
        }
    ],
    "daytime_minutes": 5.0,
    "nighttime_minutes": 2.0,
    "notes": "Biggest game to date, theoretical 50% winrate for mafia (by random chance)",
    "preparation_command": ".\\prepare_config.py -o openai_16_16_2 -p 16 -l 16 -m 2 -dt 5 -nt 2"
}